{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE7124c Lab - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. For more details - www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''\n",
    "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAt Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\nAt Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nAt Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.', 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.', 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.', 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.', 'My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.', 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.', 'At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.', 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = sent_tokenize(string)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "My companion Mr. Alfred sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokens:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'Mr.', 'Alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.', 'At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize - splits strings to words and seperates punctuations also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular-Expression Tokenizers\n",
    "\n",
    "A RegexpTokenizer splits a string into substrings using a regular expression. For example, the following tokenizer forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences:('\\w+|$[\\d.]+|\\S+') For more information or different variations - http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize http://www.nltk.org/howto/tokenize.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions \n",
    "1. \\d - Matches any decimal digit; this is equivalent to the class [0-9].\n",
    "2. \\D - Matches any non-digit character; this is equivalent to the class [^0-9].\n",
    "3. \\s - Matches any whitespace character; this is equivalent to the class \n",
    "4. \\S - Matches any non-whitespace character; this is equivalent to the class\n",
    "5. \\w - Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
    "6. \\W - Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
    "\n",
    "The special characters are:\n",
    "\n",
    "7. '.' - (Dot.) In the default mode, this matches any character except a newline. If the DOTALL flag has been specified, this             matches any character including a newline.\n",
    "8. '^' - (Caret.) Matches the start of the string, and in MULTILINE mode also matches immediately after each newline.\n",
    "9. '$' - Matches the end of the string or just before the newline at the end of the string, and in MULTILINE mode also matches            before a newline. \n",
    "10. '*' - Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. ab*             will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s.\n",
    "11. '+' - Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-             zero number of ‘b’s; it will not match just ‘a’.\n",
    "12. '?' - Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. ab? will match either ‘a’ or ‘ab’.\n",
    "13. '\\' - Either escapes special characters (permitting you to match characters like '*', '?', and so forth), or signals a                 special sequence; special sequences are discussed below.\n",
    "14. [] - Used to indicate a set of characters. In a set: Example: Characters can be listed individually, e.g. [amk] will match            'a', 'm', or 'k'.\n",
    "15. '|' - A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B. An arbitrary               number of REs can be separated by the '|' in this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTokenizer(pattern='\\\\w+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)\n",
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'My', 'companion', 'Mr', 'Alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'Suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows', 'At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "tokenizer = regextoken('\\w+')\n",
    "print(tokenizer)\n",
    "tokens = tokenizer.tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', ',', ',', '?']\n"
     ]
    }
   ],
   "source": [
    "test = (\"Alas, it has not rained today. When, do you think, \"\"will it rain again?\")\n",
    "tokenizer = regextoken('[,.?!\"]')\n",
    "\n",
    "tokens_test = tokenizer.tokenize(test)\n",
    "print(tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fleecy', 'trees', 'green', 'between', 'sweet', 'deepest', 'fleecy']\n"
     ]
    }
   ],
   "source": [
    "text_tokenizer = regextoken('\\w*ee\\w*')\n",
    "tokens_text=text_tokenizer.tokenize(string)\n",
    "print(tokens_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Examples of Regular Expressions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'N', 'S', 'O', 'F', 'E', 'h', 'a', 's', 'a', 'd', 'a', 't', 'a', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'c', 'l', 'a', 's', 's', 'o', 'n', '2', '9', '1', '1', '2', '0', '1', '8']\n",
      "['INSFOE', 'has', 'a', 'data', 'science', 'class', 'on', '29', '11', '2018']\n",
      "['INSOFE']\n",
      "['29-11-2018']\n",
      "['INSOFE']\n",
      "['29', '11', '20', '18']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "result=re.findall('\\w','INSOFE has a data science class on 29-11-2018')\n",
    "print(result)\n",
    "\n",
    "result_2=re.findall('\\w+','INSFOE has a data science class on 29-11-2018')\n",
    "print(result_2)\n",
    "\n",
    "result_3=re.findall('^\\w+','INSOFE has a data science class on 29-11-2018')\n",
    "print(result_3)\n",
    "\n",
    "result_4=re.findall('\\d+-\\d+-\\d{4}','INSOFE has a data science class on 29-11-2018')\n",
    "print(result_4)\n",
    "\n",
    "result_5=re.findall('[A-Z]+','INSOFE has a data science class on 29-11-2018')\n",
    "print(result_5)\n",
    "\n",
    "\n",
    "result_6=re.findall('[A-Za-z0-9][0-9]','INSOFE has a data science class on 29-11-2018')\n",
    "print(result_6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'mr', 'alfred', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows', 'at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens] # Converting list of token to lower case\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory.To check the list of stopwords you can type the following commands in the python shell.\n",
    "\n",
    "Note: You can even modify the list by adding words of your choice in the english .txt. file in the stopwords directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'mr', 'alfred', 'sat', 'front', 'trap', 'arms', 'folded', 'hat', 'pulled', 'eyes', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadows', 'waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "tokens = [token for token in tokens if token not in stop]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmers and Lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers vs. Lemmatizers\n",
    "* Both stemmers and lemmatizers try to bring inflected words to the same form\n",
    "* Stemmers use an algorithmic approach of removing prefixes and suffixes. The result might not be an actual dictionary word.\n",
    "* Lemmatizers use a corpus. The result is always a dictionary word.\n",
    "* Lemmatizers need extra info about the part of speech they are processing. “Calling” can be either a verb or a noun (the calling)\n",
    "* Stemmers are faster than lemmatizers\n",
    "\n",
    "When to use stemmers and when to use lemmatizers? few guidelines:\n",
    "* If speed is important, use stemmers (lemmatizers have to search through a corpus while stemmers do simple operations on a string)\n",
    "* If you just want to make sure that the system you are building is tolerant to inflections, use stemmers (If you query for “best bar in New York”, you’d accept an article on “Best bars in New York 2016″)\n",
    "* If you need the actual dictionary word, use a lemmatizer. (for example, if you are building a natural language generation system)\n",
    "\n",
    "How do stemmers work?\n",
    "\n",
    "* Stemmers are extremely simple to use and very fast. They usually are the preferred choice. They work by applying different transformation rules on the word until no other transformation can be applied.\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english')\n",
    "print snow.stem('getting')      # get\n",
    "print snow.stem('rabbits')      # rabbit\n",
    "print snow.stem('xyzing')       # xyze - it even works on non words!\n",
    "print snow.stem('quickly')      # quick\n",
    "print snow.stem('slowly')       # slowli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing.\n",
    "\n",
    "WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. However, there are some important distinctions. First, WordNet interlinks not just word forms—strings of letters—but specific senses of words. As a result, words that are found in close proximity to one another in the network are semantically disambiguated. Second, WordNet labels the semantic relations among words, whereas the groupings of words in a thesaurus does not follow any explicit pattern other than meaning similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'be', 'be', 'be', 'be', 'be']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizers\n",
    "tokens_new = [\"am\", \"be\", \"are\", \"is\", \"was\", \"were\"]\n",
    "lmtzr = WordNetLemmatizer()\n",
    "tokens_new = [lmtzr.lemmatize(token,pos = \"v\") for token in tokens_new]\n",
    "print(tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "#Stemmers\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "snowball=SnowballStemmer('english')\n",
    "print(snowball.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress\n",
      "poni\n",
      "cat\n",
      "agre\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem('caresses'))\n",
    "print(porter.stem('ponies'))\n",
    "print(porter.stem('cats'))\n",
    "print(porter.stem('agreed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('computer'))\n",
    "print(porter.stem('computer'))\n",
    "print(snowball.stem('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', 'tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'mr', 'alfred', 'sat', 'front', 'trap', 'arm', 'folded', 'hat', 'pulled', 'eye', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadow', 'waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heaven']\n"
     ]
    }
   ],
   "source": [
    "lmtzr_tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(lmtzr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortun', 'catch', 'train', 'leatherhead', 'hire', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'love', 'surrey', 'lane', 'perfect', 'day', 'bright', 'sun', 'fleeci', 'cloud', 'heaven', 'tree', 'waysid', 'hedg', 'throw', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strang', 'contrast', 'sweet', 'promis', 'spring', 'sinist', 'quest', 'upon', 'engag', 'companion', 'mr', 'alfr', 'sat', 'front', 'trap', 'arm', 'fold', 'hat', 'pull', 'eye', 'chin', 'sunk', 'upon', 'breast', 'buri', 'deepest', 'thought', 'suddenli', 'howev', 'start', 'tap', 'shoulder', 'point', 'meadow', 'waterloo', 'fortun', 'catch', 'train', 'leatherhead', 'hire', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'love', 'surrey', 'lane', 'perfect', 'day', 'bright', 'sun', 'fleeci', 'cloud', 'heaven']\n"
     ]
    }
   ],
   "source": [
    "porter_tokens = [porter.stem(token) for token in tokens]\n",
    "print(porter_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you classify or group your documents/ tweets based on these? Do single words make sense? Need to capture the context between words. Hence, we have to use ngrams to capture two or three or four words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('waterloo', 'fortun', 'catch')\n",
      "('fortun', 'catch', 'train')\n",
      "('catch', 'train', 'leatherhead')\n",
      "('train', 'leatherhead', 'hire')\n",
      "('leatherhead', 'hire', 'trap')\n",
      "('hire', 'trap', 'station')\n",
      "('trap', 'station', 'inn')\n",
      "('station', 'inn', 'drove')\n",
      "('inn', 'drove', 'four')\n",
      "('drove', 'four', 'five')\n",
      "('four', 'five', 'mile')\n",
      "('five', 'mile', 'love')\n",
      "('mile', 'love', 'surrey')\n",
      "('love', 'surrey', 'lane')\n",
      "('surrey', 'lane', 'perfect')\n",
      "('lane', 'perfect', 'day')\n",
      "('perfect', 'day', 'bright')\n",
      "('day', 'bright', 'sun')\n",
      "('bright', 'sun', 'fleeci')\n",
      "('sun', 'fleeci', 'cloud')\n",
      "('fleeci', 'cloud', 'heaven')\n",
      "('cloud', 'heaven', 'tree')\n",
      "('heaven', 'tree', 'waysid')\n",
      "('tree', 'waysid', 'hedg')\n",
      "('waysid', 'hedg', 'throw')\n",
      "('hedg', 'throw', 'first')\n",
      "('throw', 'first', 'green')\n",
      "('first', 'green', 'shoot')\n",
      "('green', 'shoot', 'air')\n",
      "('shoot', 'air', 'full')\n",
      "('air', 'full', 'pleasant')\n",
      "('full', 'pleasant', 'smell')\n",
      "('pleasant', 'smell', 'moist')\n",
      "('smell', 'moist', 'earth')\n",
      "('moist', 'earth', 'least')\n",
      "('earth', 'least', 'strang')\n",
      "('least', 'strang', 'contrast')\n",
      "('strang', 'contrast', 'sweet')\n",
      "('contrast', 'sweet', 'promis')\n",
      "('sweet', 'promis', 'spring')\n",
      "('promis', 'spring', 'sinist')\n",
      "('spring', 'sinist', 'quest')\n",
      "('sinist', 'quest', 'upon')\n",
      "('quest', 'upon', 'engag')\n",
      "('upon', 'engag', 'companion')\n",
      "('engag', 'companion', 'mr')\n",
      "('companion', 'mr', 'alfr')\n",
      "('mr', 'alfr', 'sat')\n",
      "('alfr', 'sat', 'front')\n",
      "('sat', 'front', 'trap')\n",
      "('front', 'trap', 'arm')\n",
      "('trap', 'arm', 'fold')\n",
      "('arm', 'fold', 'hat')\n",
      "('fold', 'hat', 'pull')\n",
      "('hat', 'pull', 'eye')\n",
      "('pull', 'eye', 'chin')\n",
      "('eye', 'chin', 'sunk')\n",
      "('chin', 'sunk', 'upon')\n",
      "('sunk', 'upon', 'breast')\n",
      "('upon', 'breast', 'buri')\n",
      "('breast', 'buri', 'deepest')\n",
      "('buri', 'deepest', 'thought')\n",
      "('deepest', 'thought', 'suddenli')\n",
      "('thought', 'suddenli', 'howev')\n",
      "('suddenli', 'howev', 'start')\n",
      "('howev', 'start', 'tap')\n",
      "('start', 'tap', 'shoulder')\n",
      "('tap', 'shoulder', 'point')\n",
      "('shoulder', 'point', 'meadow')\n",
      "('point', 'meadow', 'waterloo')\n",
      "('meadow', 'waterloo', 'fortun')\n",
      "('waterloo', 'fortun', 'catch')\n",
      "('fortun', 'catch', 'train')\n",
      "('catch', 'train', 'leatherhead')\n",
      "('train', 'leatherhead', 'hire')\n",
      "('leatherhead', 'hire', 'trap')\n",
      "('hire', 'trap', 'station')\n",
      "('trap', 'station', 'inn')\n",
      "('station', 'inn', 'drove')\n",
      "('inn', 'drove', 'four')\n",
      "('drove', 'four', 'five')\n",
      "('four', 'five', 'mile')\n",
      "('five', 'mile', 'love')\n",
      "('mile', 'love', 'surrey')\n",
      "('love', 'surrey', 'lane')\n",
      "('surrey', 'lane', 'perfect')\n",
      "('lane', 'perfect', 'day')\n",
      "('perfect', 'day', 'bright')\n",
      "('day', 'bright', 'sun')\n",
      "('bright', 'sun', 'fleeci')\n",
      "('sun', 'fleeci', 'cloud')\n",
      "('fleeci', 'cloud', 'heaven')\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "tmp = 0\n",
    "for ngram in nltk.ngrams(porter_tokens, 3):\n",
    "    print (ngram,)\n",
    "    tmp += 1\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('waterloo', 'fortun'), 2),\n",
       " (('fortun', 'catch'), 2),\n",
       " (('catch', 'train'), 2),\n",
       " (('train', 'leatherhead'), 2),\n",
       " (('leatherhead', 'hire'), 2),\n",
       " (('hire', 'trap'), 2),\n",
       " (('trap', 'station'), 2),\n",
       " (('station', 'inn'), 2),\n",
       " (('inn', 'drove'), 2),\n",
       " (('drove', 'four'), 2),\n",
       " (('four', 'five'), 2),\n",
       " (('five', 'mile'), 2),\n",
       " (('mile', 'love'), 2),\n",
       " (('love', 'surrey'), 2),\n",
       " (('surrey', 'lane'), 2),\n",
       " (('lane', 'perfect'), 2),\n",
       " (('perfect', 'day'), 2),\n",
       " (('day', 'bright'), 2),\n",
       " (('bright', 'sun'), 2),\n",
       " (('sun', 'fleeci'), 2),\n",
       " (('fleeci', 'cloud'), 2),\n",
       " (('cloud', 'heaven'), 2),\n",
       " (('heaven', 'tree'), 1),\n",
       " (('tree', 'waysid'), 1),\n",
       " (('waysid', 'hedg'), 1),\n",
       " (('hedg', 'throw'), 1),\n",
       " (('throw', 'first'), 1),\n",
       " (('first', 'green'), 1),\n",
       " (('green', 'shoot'), 1),\n",
       " (('shoot', 'air'), 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in nltk.ngrams(porter_tokens, 2):\n",
    "    ngram_freq[ngram] += 1\n",
    "ngram_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the above commands into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [tokenizer.tokenize(sentence) for sentence in sentences]#list of lists\n",
    "    tokens = [] # initialising variable\n",
    "    for sentence in sentence_tokens:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop:\n",
    "                sent.append(word.lower())\n",
    "        tokens.append(sent)\n",
    "    ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ##tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens\n",
    "\n",
    "def process_ngrams(input_sentence_tokens):\n",
    "    ngram_list = []\n",
    "    for sentence in input_sentence_tokens:\n",
    "        ngram_sent = nltk.ngrams(sentence, 2)\n",
    "        ngram_list = ngram_list + list(ngram_sent)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[',', '.'], [',', '.'], [',', '.'], ['.'], ['.', ',', ',', ',', ',', '.'], [',', ',', ',', ',', '.'], [',', '.'], [',', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = process_text(string)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', '.'), (',', '.'), (',', '.'), ('.', ','), (',', ','), (',', ','), (',', ','), (',', '.'), (',', ','), (',', ','), (',', ','), (',', '.'), (',', '.'), (',', '.')]\n"
     ]
    }
   ],
   "source": [
    "string_ngrams = process_ngrams(sentence_tokens)\n",
    "print(string_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((',', '.'), 7), ((',', ','), 6), (('.', ','), 1)]\n"
     ]
    }
   ],
   "source": [
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in string_ngrams:\n",
    "    ngram_freq[ngram] += 1\n",
    "print(ngram_freq.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-831d8f73e8c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud().generate(string)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
